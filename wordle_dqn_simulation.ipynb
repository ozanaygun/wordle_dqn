{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9ec315e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "class WordleEnv:\n",
    "    def __init__(self, word_length=5, max_attempts=6, subset_size=None):\n",
    "        #Length of wordle words, 5\n",
    "        self.word_length = word_length\n",
    "        # Possible number of attempts in wordle, 6\n",
    "        self.max_attempts = max_attempts\n",
    "        # We will store the target word in this\n",
    "        self.target_word = ''\n",
    "        # Initializations for the attempts, we will update this when we reset the environment\n",
    "        self.attempts_left = 0\n",
    "        self.attempts = 0\n",
    "        # Current actions will be stored in this variable\n",
    "        self.current_guess = ''\n",
    "        # Opening the txt file containing possible words and get a random subset of them if necessary\n",
    "        with open('previous_wordles.txt', 'r') as f:\n",
    "        #with open('wordle_words.txt', 'r') as f:\n",
    "            words = [word.strip().upper() for word in f.readlines() if len(word.strip()) == word_length]\n",
    "            if subset_size is not None:\n",
    "                words = self.get_random_subset(words, subset_size)\n",
    "            self.words = words\n",
    "        # State space has 78 dimensions (3 for each letter, gray, yellow, and green states)\n",
    "        self.state_size = 78\n",
    "        # Possible actions are the number of words in the dataset\n",
    "        self.action_size = len(self.words)\n",
    "        # Current state starts as all zeros one hot encoded matrix, then it will be built after each move\n",
    "        self.current_state = np.zeros(self.state_size, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "    # This function removes incompatible words based on current guesses.\n",
    "    def remove_incompatible_words(self, current_guess):\n",
    "        new_available_actions = []\n",
    "        for i in self.available_actions:\n",
    "            word = self.words[i]\n",
    "            compatible = True\n",
    "            for idx, (guess_char, target_char) in enumerate(zip(current_guess, self.target_word)):\n",
    "                if guess_char == target_char and word[idx] != guess_char:\n",
    "                    compatible = False\n",
    "                    break\n",
    "                elif guess_char != target_char and word[idx] == guess_char:\n",
    "                    compatible = False\n",
    "                    break\n",
    "            if compatible:\n",
    "                new_available_actions.append(i)\n",
    "\n",
    "        # Ensure at least one word is left in the available word list\n",
    "        if len(new_available_actions) > 0:\n",
    "            self.available_actions = new_available_actions\n",
    "\n",
    "    # This function masks action for the incompatible actions.\n",
    "    def mask_action(self, action):\n",
    "        self.available_actions.remove(action)\n",
    "    \n",
    "    # This function gets a random subset of words\n",
    "    def get_random_subset(self, words, subset_size):\n",
    "        return random.sample(words, subset_size)\n",
    "    \n",
    "    # This function chooses a random number between 0 and length of dataset, which will be transformed into word based on the index.\n",
    "    def get_random_action(self):\n",
    "        return random.randint(0, self.action_size - 1)\n",
    "\n",
    "    # Before starting each episode, the environment is resetted to give the initial conditions.\n",
    "    def reset(self):\n",
    "        self.target_word = random.choice(self.words)\n",
    "        self.attempts_left = self.max_attempts\n",
    "        self.attempts = 0\n",
    "        self.current_guess = '_' * self.word_length\n",
    "        self.available_actions = list(range(self.action_size))\n",
    "        #self.available_actions = self.words\n",
    "        self.current_state = np.zeros(self.state_size, dtype=np.float32)\n",
    "        \n",
    "        return self.current_state\n",
    "\n",
    "    # Each time we make an action (make a guess), we check how many of the letters are correct.\n",
    "    def step(self, action):\n",
    "        self.current_guess = self.words[action]\n",
    "        self.mask_action(action)  # Mask the taken action\n",
    "        self.attempts += 1\n",
    "        reward = 0\n",
    "        done = False\n",
    "        # If the guess is correct, +10 reward.\n",
    "        if self.current_guess == self.target_word:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        # If some of the letters are correct, give intermediate reward for the number of correct letters [1,4]\n",
    "        else:\n",
    "            correct_letters = sum([1 for guessed_letter, target_letter in zip(self.current_guess, self.target_word) if guessed_letter == target_letter])\n",
    "            reward = 1 * correct_letters\n",
    "            \n",
    "            self.attempts_left -= 1\n",
    "            # If there is no attempts left, unsuccessful, -10 reward.\n",
    "            if self.attempts_left <= 0:\n",
    "                reward = -10\n",
    "                done = True\n",
    "                \n",
    "        self.remove_incompatible_words(self.current_guess)\n",
    "\n",
    "        return self.get_state(), reward, done, {}\n",
    "    \n",
    "    # In each turn, get the new state based on the correctness of the letters\n",
    "    def get_state(self):\n",
    "        #state = np.zeros(self.state_size, dtype=np.float32)\n",
    "        state = self.current_state\n",
    "        # Check each letter of the guess\n",
    "        for idx, letter in enumerate(self.current_guess):\n",
    "            # If correct location and letter (green), that is allocated for 0,25\n",
    "            if letter == self.target_word[idx]:\n",
    "                state[(ord(letter) - 65)] = 1\n",
    "            # If only correct letter (yellow), allocated for second 26 indices.\n",
    "            elif letter in self.target_word:\n",
    "                state[(ord(letter) - 65) + 26] = 1\n",
    "            # If the letter is not in the word, allocated for the last 26 indices.\n",
    "            else:\n",
    "                state[(ord(letter) - 65) + 26*2] = 1\n",
    "        return state\n",
    "\n",
    "    # Printing output purposes.\n",
    "    def render(self):\n",
    "        print(f\"Current guess: {self.current_guess}\")\n",
    "        print(f\"Target word: {self.target_word}\")\n",
    "        print(f\"Attempts left: {self.attempts_left}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e8ee1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the environment (input subset size if necessary)\n",
    "size = None\n",
    "env = WordleEnv(subset_size=size)  # A random subset of size words will be used\n",
    "\n",
    "# CUDA purposes for M1 Pro.\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d933395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay memory object for replaying samples from the memory.\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ef085d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent's network\n",
    "# 3 Layers, starts with the state space shape and ends with action space shape.\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "89a188c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 150000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_size\n",
    "# Get the number of state observations\n",
    "state = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(env.state_size, env.action_size).to(device)\n",
    "target_net = DQN(env.state_size, env.action_size).to(device)\n",
    "\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Optimizer initialization\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "# Epsilon greedy action selection\n",
    "# Gradually decrease epsilon\n",
    "# If epsilon is greater than the random sample, take random action\n",
    "# Otherwise, take the action that gives the most Q value.\n",
    "def select_action(state, available_actions, action_size):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    global eps_threshold\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Create a mask tensor for previously chosen actions\n",
    "            mask = torch.full((1, action_size), -float('inf'), device=device)\n",
    "            for idx in available_actions:\n",
    "                mask[0, idx] = 0\n",
    "\n",
    "            # Add the mask to the DQN output and select the maximum value\n",
    "            masked_output = policy_net(state) + mask\n",
    "            return masked_output.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.choice(available_actions)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a412a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cb479d75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/300000, Attempts: 6, Reward: 10\n",
      "Episode: 1000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 2000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 3000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 4000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 5000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 6000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 7000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 8000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 9000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 10000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 11000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 12000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 13000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 14000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 15000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 16000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 17000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 18000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 19000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 20000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 21000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 22000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 23000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 24000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 25000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 26000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 27000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 28000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 29000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 30000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 31000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 32000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 33000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 34000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 35000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 36000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 37000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 38000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 39000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 40000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 41000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 42000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 43000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 44000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 45000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 46000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 47000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 48000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 49000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 50000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 51000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 52000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 53000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 54000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 55000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 56000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 57000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 58000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 59000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 60000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 61000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 62000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 63000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 64000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 65000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 66000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 67000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 68000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 69000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 70000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 71000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 72000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 73000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 74000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 75000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 76000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 77000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 78000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 79000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 80000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 81000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 82000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 83000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 84000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 85000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 86000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 87000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 88000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 89000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 90000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 91000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 92000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 93000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 94000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 95000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 96000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 97000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 98000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 99000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 100000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 101000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 102000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 103000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 104000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 105000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 106000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 107000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 108000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 109000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 110000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 111000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 112000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 113000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 114000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 115000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 116000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 117000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 118000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 119000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 120000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 121000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 122000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 123000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 124000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 125000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 126000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 127000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 128000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 129000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 130000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 131000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 132000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 133000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 134000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 135000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 136000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 137000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 138000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 139000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 140000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 141000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 142000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 143000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 144000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 145000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 146000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 147000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 148000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 149000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 150000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 151000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 152000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 153000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 154000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 155000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 156000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 157000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 158000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 159000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 160000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 161000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 162000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 163000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 164000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 165000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 166000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 167000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 168000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 169000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 170000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 171000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 172000/300000, Attempts: 3, Reward: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 173000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 174000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 175000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 176000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 177000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 178000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 179000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 180000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 181000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 182000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 183000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 184000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 185000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 186000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 187000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 188000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 189000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 190000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 191000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 192000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 193000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 194000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 195000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 196000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 197000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 198000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 199000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 200000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 201000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 202000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 203000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 204000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 205000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 206000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 207000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 208000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 209000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 210000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 211000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 212000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 213000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 214000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 215000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 216000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 217000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 218000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 219000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 220000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 221000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 222000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 223000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 224000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 225000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 226000/300000, Attempts: 2, Reward: 10\n",
      "Episode: 227000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 228000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 229000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 230000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 231000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 232000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 233000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 234000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 235000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 236000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 237000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 238000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 239000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 240000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 241000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 242000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 243000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 244000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 245000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 246000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 247000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 248000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 249000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 250000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 251000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 252000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 253000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 254000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 255000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 256000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 257000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 258000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 259000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 260000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 261000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 262000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 263000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 264000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 265000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 266000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 267000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 268000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 269000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 270000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 271000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 272000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 273000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 274000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 275000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 276000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 277000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 278000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 279000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 280000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 281000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 282000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 283000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 284000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 285000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 286000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 287000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 288000/300000, Attempts: 3, Reward: 10\n",
      "Episode: 289000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 290000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 291000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 292000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 293000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 294000/300000, Attempts: 6, Reward: -10\n",
      "Episode: 295000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 296000/300000, Attempts: 6, Reward: 10\n",
      "Episode: 297000/300000, Attempts: 5, Reward: 10\n",
      "Episode: 298000/300000, Attempts: 4, Reward: 10\n",
      "Episode: 299000/300000, Attempts: 6, Reward: -10\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300000\n",
    "average_reward = 0\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        #print(observation)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            average_reward += reward/1000\n",
    "            if(episode % 1000 == 0):\n",
    "                print(f\"Episode: {episode}/{num_episodes}, Attempts: {env.attempts}, Reward: {reward[0]}\")\n",
    "                #print(f\"Episode: {episode}/{num_episodes}, Attempts: {env.attempts}, Average Reward: {average_reward[0]}\")\n",
    "                average_reward = 0\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "332ea6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials: 1000, Success rate: 0.66, Average number of attempts: 5.09\n"
     ]
    }
   ],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1bceb60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trials with SALET start: 1000, Success rate: 0.71, Average number of attempts: 4.95\n"
     ]
    }
   ],
   "source": [
    "total_attempts = 0\n",
    "correct_guesses = 0\n",
    "\n",
    "no_test_trials = 1000\n",
    "\n",
    "eps_threshold = 1e-11 #For only exploration\n",
    "\n",
    "for episode in range(no_test_trials):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    action = torch.tensor([[345]], device=device, dtype=torch.long) #Salet start.\n",
    "    for t in count():\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        state = next_state\n",
    "        total_attempts += 1\n",
    "\n",
    "        if done:\n",
    "            if(reward[0] == 10):\n",
    "                correct_guesses += 1\n",
    "            break\n",
    "        action = select_action(state, env.available_actions, env.action_size)\n",
    "\n",
    "success_rate = correct_guesses / (no_test_trials)\n",
    "average_attempts = total_attempts / (no_test_trials)\n",
    "\n",
    "print(f\"Trials with SALET start: {no_test_trials}, Success rate: {success_rate:.2f}, Average number of attempts: {average_attempts:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f9a3e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AN INSTANT WHERE BOTH AGENT'S FIRST WORD (SLOSH) AND SALET PERFORM THE SAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "32723e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SLOSH\n",
      "Target word: THORN\n",
      "Attempts left: 5\n",
      "Current guess: BOOZY\n",
      "Target word: THORN\n",
      "Attempts left: 4\n",
      "Current guess: ATOLL\n",
      "Target word: THORN\n",
      "Attempts left: 3\n",
      "Current guess: ERODE\n",
      "Target word: THORN\n",
      "Attempts left: 2\n",
      "Current guess: THORN\n",
      "Target word: THORN\n",
      "Attempts left: 2\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "28dd14cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SALET\n",
      "Target word: THORN\n",
      "Attempts left: 5\n",
      "Current guess: KEBAB\n",
      "Target word: THORN\n",
      "Attempts left: 4\n",
      "Current guess: TRAWL\n",
      "Target word: THORN\n",
      "Attempts left: 3\n",
      "Current guess: TWINE\n",
      "Target word: THORN\n",
      "Attempts left: 2\n",
      "Current guess: THORN\n",
      "Target word: THORN\n",
      "Attempts left: 2\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'THORN'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe34a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AN INSTANT WHERE SALET FIRST WORD PERFORM BETTER THAN SLOSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "76517b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SLOSH\n",
      "Target word: FRONT\n",
      "Attempts left: 5\n",
      "Current guess: BOOZY\n",
      "Target word: FRONT\n",
      "Attempts left: 4\n",
      "Current guess: ATOLL\n",
      "Target word: FRONT\n",
      "Attempts left: 3\n",
      "Current guess: ERODE\n",
      "Target word: FRONT\n",
      "Attempts left: 2\n",
      "Current guess: GROIN\n",
      "Target word: FRONT\n",
      "Attempts left: 1\n",
      "Current guess: FRONT\n",
      "Target word: FRONT\n",
      "Attempts left: 1\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c73c7b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SALET\n",
      "Target word: FRONT\n",
      "Attempts left: 5\n",
      "Current guess: POINT\n",
      "Target word: FRONT\n",
      "Attempts left: 4\n",
      "Current guess: CHANT\n",
      "Target word: FRONT\n",
      "Attempts left: 3\n",
      "Current guess: FRONT\n",
      "Target word: FRONT\n",
      "Attempts left: 3\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'FRONT'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AN INSTANT WHERE SLOSH PERFORM BETTER THAN SALET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c2f2f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SLOSH\n",
      "Target word: CHOKE\n",
      "Attempts left: 5\n",
      "Current guess: BOOZY\n",
      "Target word: CHOKE\n",
      "Attempts left: 4\n",
      "Current guess: WROTE\n",
      "Target word: CHOKE\n",
      "Attempts left: 3\n",
      "Current guess: CHOKE\n",
      "Target word: CHOKE\n",
      "Attempts left: 3\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "78feac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current guess: SALET\n",
      "Target word: CHOKE\n",
      "Attempts left: 5\n",
      "Current guess: KEBAB\n",
      "Target word: CHOKE\n",
      "Attempts left: 4\n",
      "Current guess: TRAWL\n",
      "Target word: CHOKE\n",
      "Attempts left: 3\n",
      "Current guess: ACUTE\n",
      "Target word: CHOKE\n",
      "Attempts left: 2\n",
      "Current guess: DODGE\n",
      "Target word: CHOKE\n",
      "Attempts left: 1\n",
      "Current guess: ELOPE\n",
      "Target word: CHOKE\n",
      "Attempts left: 0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.target_word = 'CHOKE'\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = torch.tensor([[344]], device=device, dtype=torch.long) #Salet start.\n",
    "\n",
    "for t in count():\n",
    "    observation, reward, done, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    env.render()\n",
    "    if done:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    if done:\n",
    "        break\n",
    "    action = select_action(state, env.available_actions, env.action_size)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f8df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
